{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core import display as ICD\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from itertools import groupby, product\n",
    "import json\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lcm_output(input_name,folder=\"lcm_results\"):\n",
    "    \"\"\"Read and restructure LCM output file,rename columns output a df \"\"\"\n",
    "    file = f'{folder}/{input_name}'\n",
    "    df = pd.read_csv(file,header=None)\n",
    "    df.columns = [\"user_ids\",\"support\",\"itemsets\",\"period\",\"property_values\"]\n",
    "    df[\"period\"] = pd.to_datetime(df[\"period\"])\n",
    "    df[\"user_ids\"] = df.user_ids.apply(lambda x : np.array([int(z) for z in x.split(\" \") if z != \"\"]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_lcm_output_total(input_name,folder=\"../lcm_results\"):\n",
    "    \"\"\"Read and restructure LCM output file,rename columns output a df \"\"\"\n",
    "    file = f'{folder}/{input_name}'\n",
    "    df = pd.read_csv(file,header=None)\n",
    "    df.columns = [\"user_ids\",\"support\",\"itemsets\",\"period\",\"property_values\"]\n",
    "    df[\"period\"] = pd.to_datetime(df[\"period\"])\n",
    "    df[\"user_ids\"] = df.user_ids.apply(lambda x : np.array([int(z.replace('\"',\"\")) for z in x[1:-1].split(\",\") if z != \"\"]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for echarts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_links(x):\n",
    "    res = []\n",
    "    for i in x[0]:\n",
    "        for idx in range(len(i)-1):\n",
    "            res.append([i[idx],i[idx+1],x[\"index\"]])\n",
    "    return res\n",
    "\n",
    "def make_links(e,index):\n",
    "    \"\"\"Product of groups ids for each two consecutive groups periods\"\"\"\n",
    "    prev = e[0]\n",
    "    for i in e[1:]:\n",
    "        yield from product(prev,i,[index])\n",
    "        prev = i\n",
    "\n",
    "def extract_demographics(input_file):\n",
    "    demographics = re.findall(\"\\[([A-Z|a-z|_]+),?([A-Z|a-z|_]+)?\\]\",input_file)[0]\n",
    "    return  [i for i in demographics if i !=\"\"]\n",
    "    \n",
    "def sankey_preprocessing(input_file,stats_folder='../plots/stats',encoders_folder=\"../plots/encoders\",links_folder=\"../plots/links\",groups_folder='../plots/groups',users_demographics = [\"DEPARTEMENT\",\"SEX\",\"AGE\"],groups_demographics=[\"STATION_MGT_TYPE\",\"DEPARTEMENT\"],users_consecutive_apparition=3):\n",
    "#     if \"M-10-[2-5000]-[AGE]-lcm.out\" in input_file :\n",
    "#         return\n",
    "    \n",
    "    demographics = extract_demographics(input_file)\n",
    "    users = pd.read_csv(\"../datasets/Total/users.csv\",sep=\";\")\n",
    "    df = read_lcm_output_total(input_file).sort_values(\"period\").reset_index(drop=True)\n",
    "    \n",
    "    file = f'../plots/links/{input_file}'\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    _df = mlb.fit_transform(df.user_ids.tolist()).astype(bool)\n",
    "    _df = pd.DataFrame(_df.toarray(),columns=mlb.classes_)\n",
    "    \n",
    "    e = _df.sum()\n",
    "    _df = _df[e[e>1].index]\n",
    "    _df  = _df.T.apply(lambda x : np.where(x)[0],axis=1)\n",
    "    e = _df.to_frame()[0].apply(lambda x: list(list(z) for idx,z in groupby(x,lambda y: df.iloc[y].period)))\n",
    "    e = e[e.apply(lambda x:len(x))>users_consecutive_apparition]\n",
    "    \n",
    "    res = []\n",
    "    e.to_frame().reset_index().apply(lambda x: [res.append(i) for i in make_links(x[0],x[\"index\"])],axis=1)\n",
    "    links = pd.DataFrame(res)\n",
    "    links.columns = [\"source\",\"target\",\"user_id\"]\n",
    "    links.groupby([\"source\",\"target\"])[\"user_id\"].apply(lambda x: ','.join(str(i) for i in x)).to_frame().to_csv(file)\n",
    "\n",
    "    # Users demographics stats  \n",
    "    file = f'../plots/stats/users/{input_file}'\n",
    "    stats = {}\n",
    "    users_stats = links[[\"user_id\"]].drop_duplicates().merge(users,left_on=\"user_id\",right_on=\"CUST_ID\")[users.columns]\n",
    "    for i in users_demographics:\n",
    "        b = users_stats.groupby(i).apply(lambda x: {\"name\":x[i].unique()[0],\"value\":x.CUST_ID.shape[0],\"users\":\",\".join(str(i) for i in x.CUST_ID)}).values\n",
    "        stats[i] = b.tolist()\n",
    "        \n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(stats, outfile)\n",
    "  \n",
    "    \n",
    "    # Users\n",
    "    file = f'../plots/users/{input_file}'\n",
    "    users_stats.to_csv(file)\n",
    "    \n",
    "    # filter groups to the ones appearing in the links\n",
    "    file = f'{groups_folder}/{input_file}'\n",
    "    df_reduced_filtred = df.loc[np.unique(np.union1d(links.source.unique(),links.target.unique()))].dropna()\n",
    "    df_reduced_filtred['depth'] = le.fit_transform(df_reduced_filtred.period)/df_reduced_filtred.period.nunique()\n",
    "    df_reduced_filtred['size'] = df_reduced_filtred.user_ids.apply(lambda x : len(x))\n",
    "    if len(demographics)==1:\n",
    "        df_reduced_filtred[demographics[0]]= df_reduced_filtred.property_values\n",
    "    else:\n",
    "        df_reduced_filtred[demographics]= df_reduced_filtred.property_values.str.split(\"_\",expand=True)\n",
    "        \n",
    "    # Encoding items to their initial ID + adding names\n",
    "    items = pd.read_csv(\"../datasets/Total/items.csv\",encoding=\"latin\",sep=\";\")\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(f'{encoders_folder}/{input_file}.npy')\n",
    "    return encoder,df_reduced_filtred\n",
    "    df_reduced_filtred[\"itemsets_name\"] = df_reduced_filtred[\"itemsets\"].apply(lambda x : encoder.inverse_transform([int(i) for i in x.split()]))\n",
    "    \n",
    "    z = df_reduced_filtred[\"itemsets_name\"] .apply(lambda x : [items[items.ARTICLE_ID==i].DESCRIPTION.tolist() for i in x ]).to_frame()\n",
    "    df_reduced_filtred[\"itemsets_name\"] = z.apply(lambda x : \"; \".join([i for i in x[0][0]]),axis=1)\n",
    "    df_reduced_filtred[\"itemsets\"] = df_reduced_filtred[\"itemsets\"].apply(lambda x :\":\".join(str(i) for i in encoder.inverse_transform([int(i) for i in x.split()])))\n",
    "    \n",
    "   \n",
    "    df_reduced_filtred.to_csv(file)\n",
    "    \n",
    "    # Groups demographics stats \n",
    "    file = f'../plots/stats/groups/{input_file}'\n",
    "    stats = {}\n",
    "    for i in np.intersect1d(groups_demographics,demographics):\n",
    "        b = df_reduced_filtred.groupby(i).apply(lambda x : {\"name\":x[i].unique()[0],\"value\":x.index.shape[0],\"groups\":\",\".join(str(i) for i in x.index)}).values\n",
    "        stats[i]=str(b.tolist())\n",
    "        \n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(stats, outfile)\n",
    "    \n",
    "    \n",
    "    print(\"Done\",input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9M-5-[1-2001]-[a]-lcm.out ['a']\n"
     ]
    }
   ],
   "source": [
    "input_file = \"9M-5-[1-2001]-[a]-lcm.out\"\n",
    "for i in [\"9M-5-[1-2001]-[a]-lcm.out\"]:\n",
    "    print(i,extract_demographics(i))\n",
    "    try:\n",
    "        e = sankey_preprocessing(i,users_consecutive_apparition=2)\n",
    "    except Exception as e:\n",
    "        print(\"Error\",i,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b.apply(lambda x : a.inverse_transform([int(i) for i in x[\"itemsets\"].split()]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../datasets/Total/items.csv\",encoding=\"latin\",sep=\";\").drop_duplicates(\"ARTICLE_ID\")\n",
    "items = items.set_index(\"ARTICLE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_mapping(x):\n",
    "    try:\n",
    "        return items.loc[x].DESCRIPTION\n",
    "    except:\n",
    "        print(\"Item not found\",x)\n",
    "        return \"No Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item not found 6206300138427\n",
      "Item not found 6206300138427\n",
      "Item not found 6206300138427\n",
      "Item not found 100\n",
      "Item not found 3461039\n",
      "Item not found 3461039\n",
      "Item not found 3461039\n",
      "Item not found 1\n",
      "Item not found 6206300218860\n",
      "Item not found 6206300218860\n",
      "Item not found 5401046\n",
      "Item not found 5401046\n",
      "Item not found 5401046\n",
      "Item not found 99990000\n",
      "Item not found 3450137\n",
      "Item not found 5401046\n",
      "Item not found 5401046\n",
      "Item not found 5401046\n",
      "Item not found 6206300138922\n",
      "Item not found 6206300138922\n",
      "Item not found 5401046\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 6206300219508\n",
      "Item not found 12520046\n",
      "Item not found 5401046\n",
      "Item not found 451\n",
      "Item not found 12520039\n",
      "Item not found 24\n",
      "Item not found 3461040\n",
      "Item not found 3217770314245\n",
      "Item not found 5450138\n",
      "Item not found 3456774836920\n",
      "Item not found 3412051\n",
      "Item not found 3461041\n",
      "Item not found 7804200131952\n",
      "Item not found 7804200131952\n",
      "Item not found 26\n",
      "Item not found 7822500384762\n",
      "Item not found 7804200142125\n",
      "Item not found 7804200142125\n",
      "Item not found 7804200142125\n",
      "Item not found 7822500172819\n",
      "Item not found 3412051\n",
      "Item not found 3461039\n",
      "Item not found 3412051\n",
      "Item not found 3412051\n",
      "Item not found 6206300138427\n",
      "Item not found 6206300138427\n",
      "Item not found 3450137\n",
      "Item not found 3450137\n",
      "Item not found 3450137\n",
      "Item not found 100\n",
      "Item not found 3401043\n",
      "Item not found 3068320120287\n",
      "Item not found 6206300219508\n",
      "Item not found 3461039\n",
      "Item not found 1\n",
      "Item not found 3461039\n",
      "Item not found 3461039\n",
      "Item not found 99990000\n",
      "Item not found 99990000\n",
      "Item not found 6206300219508\n",
      "Item not found 3502110009449\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 451\n",
      "Item not found 3502110009449\n",
      "Item not found 3502110009449\n",
      "Item not found 2181214014064\n",
      "Item not found 2181214014064\n",
      "Item not found 2181214014064\n",
      "Item not found 2181214014064\n",
      "Item not found 2181214014064\n",
      "Item not found 451\n",
      "Item not found 7612720841042\n",
      "Item not found 7822500379676\n",
      "Item not found 3461040\n",
      "Item not found 11679\n",
      "Item not found 3502110009449\n",
      "Item not found 3461039\n",
      "Item not found 106\n",
      "Item not found 7804200142125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3                          [CR SANDWHICH OCEANIQUE (new)]\n",
       "5                                        [RED BULL 355ML]\n",
       "9       [CR COOKIE CHOCOLAT LAIT 103 GR, CR SAND PREMIUM]\n",
       "11      [CR COOKIE CHOCOLAT LAIT 103 GR, CR SANDWICH C...\n",
       "13                             [No Name, RECHARGE LAVAGE]\n",
       "                              ...                        \n",
       "1581                                    [DOLE PECHE SIRO]\n",
       "1588                         [CR TARTE CITRON MERINGUÉET]\n",
       "1589               [Lun Salade Accueil, LU POMMES FRITES]\n",
       "1591                                            [No Name]\n",
       "1592                   [HOLLYW.BLANCH.MENTH. POLAIRE 14G]\n",
       "Length: 455, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.apply(lambda x : [items_mapping(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARTICLE_ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3425909001007</th>\n",
       "      <td>RECHARGE LAVAGE TW - A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425909001083</th>\n",
       "      <td>REC LAVAGE PJE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124480186188</th>\n",
       "      <td>SCHWEPPES MOJITO 50CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559750000893</th>\n",
       "      <td>NEOCLEAN JANTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410041005707</th>\n",
       "      <td>LU PIMS ORANGE 150G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011449248370</th>\n",
       "      <td>TOTAL LINGET.E.CUIRX16+8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700232616969</th>\n",
       "      <td>CHARGEUR UNIVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609200003339</th>\n",
       "      <td>NA FRUIT CHOC F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700232615771</th>\n",
       "      <td>ADAPT UNIVERSELLE PAYS EU 10A MAX 250V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203955353022</th>\n",
       "      <td>TAPIS RNLT 4P N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DESCRIPTION\n",
       "ARTICLE_ID                                           \n",
       "3425909001007                  RECHARGE LAVAGE TW - A\n",
       "3425909001083                          REC LAVAGE PJE\n",
       "3124480186188                   SCHWEPPES MOJITO 50CL\n",
       "3559750000893                         NEOCLEAN JANTES\n",
       "5410041005707                     LU PIMS ORANGE 150G\n",
       "...                                               ...\n",
       "3011449248370                TOTAL LINGET.E.CUIRX16+8\n",
       "3700232616969                         CHARGEUR UNIVER\n",
       "3609200003339                         NA FRUIT CHOC F\n",
       "3700232615771  ADAPT UNIVERSELLE PAYS EU 10A MAX 250V\n",
       "5203955353022                         TAPIS RNLT 4P N\n",
       "\n",
       "[11979 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
