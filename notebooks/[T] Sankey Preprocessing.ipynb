{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core import display as ICD\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from itertools import groupby, product\n",
    "import json\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lcm_output(input_name,folder=\"lcm_results\"):\n",
    "    \"\"\"Read and restructure LCM output file,rename columns output a df \"\"\"\n",
    "    file = f'{folder}/{input_name}'\n",
    "    df = pd.read_csv(file,header=None)\n",
    "    df.columns = [\"user_ids\",\"support\",\"itemsets\",\"period\",\"property_values\"]\n",
    "    df[\"period\"] = pd.to_datetime(df[\"period\"])\n",
    "    df[\"user_ids\"] = df.user_ids.apply(lambda x : np.array([int(z) for z in x.split(\" \") if z != \"\"]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_lcm_output_total(input_name,folder=\"../lcm_results\"):\n",
    "    \"\"\"Read and restructure LCM output file,rename columns output a df \"\"\"\n",
    "    file = f'{folder}/{input_name}'\n",
    "    df = pd.read_csv(file,header=None)\n",
    "    df.columns = [\"user_ids\",\"support\",\"itemsets\",\"period\",\"property_values\"]\n",
    "    df[\"period\"] = pd.to_datetime(df[\"period\"])\n",
    "    df[\"user_ids\"] = df.user_ids.apply(lambda x : [int(z.replace('\"',\"\")) for z in x[1:-1].split(\",\") if z != \"\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for echarts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_descriptions(x,items,encoder):\n",
    "    ee = encoder.inverse_transform([int(i) for i in x.split()])\n",
    "    return items.loc[items.index.isin(ee)].DESCRIPTION.tolist()\n",
    "def format_links(x):\n",
    "    res = []\n",
    "    for i in x[0]:\n",
    "        for idx in range(len(i)-1):\n",
    "            res.append([i[idx],i[idx+1],x[\"index\"]])\n",
    "    return res\n",
    "\n",
    "def make_links(e,index):\n",
    "    \"\"\"Product of groups ids for each two consecutive groups periods\"\"\"\n",
    "    prev = e[0]\n",
    "    for i in e[1:]:\n",
    "        yield from product(prev,i,[index])\n",
    "        prev = i\n",
    "\n",
    "def extract_demographics(input_file):\n",
    "    demographics = re.findall(\"\\[([A-Z|a-z|_]+),?([A-Z|a-z|_]+)?\\]\",input_file)[0]\n",
    "    return  [i for i in demographics if i !=\"\"]\n",
    "    \n",
    "def sankey_preprocessing(input_file,stats_folder='../plots/stats',encoders_folder=\"../plots/encoders\",links_folder=\"../plots/links\",groups_folder='../plots/groups',users_demographics = [\"DEPARTEMENT\",\"SEX\",\"AGE\"],groups_demographics=[\"STATION_MGT_TYPE\",\"DEPARTEMENT\"],users_consecutive_apparition=3,remove_group_with_no_links=True):\n",
    "\n",
    "    demographics = extract_demographics(input_file)\n",
    "    users = pd.read_csv(\"../datasets/Total/users.csv\",sep=\";\")\n",
    "    df = read_lcm_output_total(input_file).sort_values(\"period\").reset_index(drop=True)\n",
    "    \n",
    "    file = f'../plots/links/{input_file}'\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    _df = mlb.fit_transform(df.user_ids.tolist()).astype(bool)\n",
    "    _df = pd.DataFrame(_df.toarray(),columns=mlb.classes_)\n",
    "    \n",
    "    e = _df.sum()\n",
    "    _df = _df[e[e>0].index]\n",
    "    _df  = _df.T.apply(lambda x : np.where(x)[0],axis=1)\n",
    "    e = _df.to_frame()[0].apply(lambda x: list(list(z) for idx,z in groupby(x,lambda y: df.iloc[y].period)))\n",
    "    e = e[e.apply(lambda x:len(x))>users_consecutive_apparition]\n",
    "    \n",
    "    res = []\n",
    "    e.to_frame().reset_index().apply(lambda x: [res.append(i) for i in make_links(x[0],x[\"index\"])],axis=1)\n",
    "    links = pd.DataFrame(res)\n",
    "    links.columns = [\"source\",\"target\",\"user_id\"]\n",
    "    links.groupby([\"source\",\"target\"])[\"user_id\"].apply(lambda x: ','.join(str(i) for i in x)).to_frame().to_csv(file)\n",
    "\n",
    "    # Users demographics stats  \n",
    "    file = f'../plots/stats/users/{input_file}'\n",
    "    stats = {}\n",
    "    users_stats = links[[\"user_id\"]].drop_duplicates().merge(users,left_on=\"user_id\",right_on=\"CUST_ID\")[users.columns]\n",
    "    for i in users_demographics:\n",
    "        b = users_stats.groupby(i).apply(lambda x: {\"name\":x[i].unique()[0],\"value\":x.CUST_ID.shape[0],\"users\":\",\".join(str(i) for i in x.CUST_ID)}).values\n",
    "        stats[i] = b.tolist()\n",
    "        \n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(stats, outfile)\n",
    "  \n",
    "    \n",
    "    # Users\n",
    "    file = f'../plots/users/{input_file}'\n",
    "    users_stats.to_csv(file)\n",
    "    \n",
    "    # filter groups to the ones appearing in the links\n",
    "    file = f'{groups_folder}/{input_file}'\n",
    "    if remove_group_with_no_links:\n",
    "        df_reduced_filtred = df.loc[np.unique(np.union1d(links.source.unique(),links.target.unique()))].dropna()\n",
    "    else: \n",
    "        df_reduced_filtred = df.dropna()\n",
    "    df_reduced_filtred['depth'] = le.fit_transform(df_reduced_filtred.period)/df_reduced_filtred.period.nunique()\n",
    "    df_reduced_filtred['size'] = df_reduced_filtred.user_ids.apply(lambda x : len(x))\n",
    "    if len(demographics)==1:\n",
    "        df_reduced_filtred[demographics[0]]= df_reduced_filtred.property_values\n",
    "    else:\n",
    "        df_reduced_filtred[demographics]= df_reduced_filtred.property_values.str.split(\"_\",expand=True)\n",
    "    \n",
    "    # Encoding items to their initial ID + adding names\n",
    "    items = pd.read_csv(\"../datasets/Total/items.csv\")\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(f'{encoders_folder}/{input_file}.npy')\n",
    "    items = items.set_index(\"ARTICLE_ID\")\n",
    "    df_reduced_filtred[\"itemset_name\"] = df_reduced_filtred[\"itemsets\"].apply(lambda x : get_articles_descriptions(x,items,encoder))\n",
    "    df_reduced_filtred.to_csv(file)\n",
    "    # Groups demographics stats \n",
    "    file = f'../plots/stats/groups/{input_file}'\n",
    "    stats = {}\n",
    "    for i in np.intersect1d(groups_demographics,demographics):\n",
    "        b = df_reduced_filtred.groupby(i).apply(lambda x : {\"name\":x[i].unique()[0],\"value\":x.index.shape[0],\"groups\":\",\".join(str(i) for i in x.index)}).values\n",
    "        stats[i]=str(b.tolist())\n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(stats, outfile)\n",
    "\n",
    "    print(\"Done\",input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9M-5-[1-2001]-[a]-lcm.out ['a']\n",
      "Done 9M-5-[1-2001]-[a]-lcm.out\n"
     ]
    }
   ],
   "source": [
    "input_file = \"9M-5-[1-2001]-[a]-lcm.out\"\n",
    "for i in [\"9M-5-[1-2001]-[a]-lcm.out\"]:\n",
    "    print(i,extract_demographics(i))\n",
    "    e = sankey_preprocessing(i,users_consecutive_apparition=1,remove_group_with_no_links=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../datasets/Total/items.csv\")\n",
    "# encoder = LabelEncoder()\n",
    "# encoders_folder=\"../plots/encoders\"\n",
    "# encoder.classes_ = np.load(f'{encoders_folder}/{input_file}.npy')\n",
    "# items.ARTICLE_ID.as\n",
    "# items = items.set_index(\"ARTICLE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_ids</th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>period</th>\n",
       "      <th>property_values</th>\n",
       "      <th>depth</th>\n",
       "      <th>size</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[100169, 100882, 102233, 103922, 105498, 10564...</td>\n",
       "      <td>1180</td>\n",
       "      <td>4112</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[149358, 335918, 490858, 503825, 507317, 59453...</td>\n",
       "      <td>9</td>\n",
       "      <td>3408</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[131823, 199655, 402629, 449166, 535449, 62564...</td>\n",
       "      <td>10</td>\n",
       "      <td>5624</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[146720, 249738, 250022, 470752, 534467, 10788...</td>\n",
       "      <td>8</td>\n",
       "      <td>6894</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[381774, 441047, 684803, 881163, 1007975, 1014...</td>\n",
       "      <td>7</td>\n",
       "      <td>4443</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>[156096, 245887, 326603, 342631, 468837, 52639...</td>\n",
       "      <td>13</td>\n",
       "      <td>1391 431</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>[123517, 197808, 257982, 315386, 329954, 35725...</td>\n",
       "      <td>17</td>\n",
       "      <td>6447</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>[197808, 329954, 387836, 470752, 844497, 890276]</td>\n",
       "      <td>6</td>\n",
       "      <td>6447 4112</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>[257982, 315386, 470752, 524845, 811849, 84449...</td>\n",
       "      <td>8</td>\n",
       "      <td>6447 4126</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>[197808, 329954, 357250, 369089, 476652, 519847]</td>\n",
       "      <td>6</td>\n",
       "      <td>6447 4125</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               user_ids  support    itemsets  \\\n",
       "0     [100169, 100882, 102233, 103922, 105498, 10564...     1180        4112   \n",
       "4     [149358, 335918, 490858, 503825, 507317, 59453...        9        3408   \n",
       "5     [131823, 199655, 402629, 449166, 535449, 62564...       10        5624   \n",
       "8     [146720, 249738, 250022, 470752, 534467, 10788...        8        6894   \n",
       "26    [381774, 441047, 684803, 881163, 1007975, 1014...        7        4443   \n",
       "...                                                 ...      ...         ...   \n",
       "1558  [156096, 245887, 326603, 342631, 468837, 52639...       13    1391 431   \n",
       "1561  [123517, 197808, 257982, 315386, 329954, 35725...       17        6447   \n",
       "1562   [197808, 329954, 387836, 470752, 844497, 890276]        6   6447 4112   \n",
       "1563  [257982, 315386, 470752, 524845, 811849, 84449...        8   6447 4126   \n",
       "1564   [197808, 329954, 357250, 369089, 476652, 519847]        6   6447 4125   \n",
       "\n",
       "         period  property_values     depth  size  a  \n",
       "0    2018-09-01                1  0.000000  1180  1  \n",
       "4    2018-09-01                1  0.000000     9  1  \n",
       "5    2018-09-01                1  0.000000    10  1  \n",
       "8    2018-09-01                1  0.000000     8  1  \n",
       "26   2018-09-01                1  0.000000     7  1  \n",
       "...         ...              ...       ...   ... ..  \n",
       "1558 2019-02-01                1  0.666667    13  1  \n",
       "1561 2019-02-01                1  0.666667    17  1  \n",
       "1562 2019-02-01                1  0.666667     6  1  \n",
       "1563 2019-02-01                1  0.666667     8  1  \n",
       "1564 2019-02-01                1  0.666667     6  1  \n",
       "\n",
       "[610 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.ARTICLE_ID = items.ARTICLE_ID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list(['2 LAMPES W5W 12V TKA FLAURAUD']),\n",
       "       list(['ASPIRATEUR LAVAGE COMPLET']), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list(['LIPTON ICE TEA 50CL']),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list(['2 BROYES DU POI']),\n",
       "       list(['ASSAINISSANT CA']), list([]), list(['LIPTON ICE TEA 50CL']),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list(['ASPERGES']), list([]), list(['ASPERGES']),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['LIPTON ICE TEA 50CL']), list([]), list([]),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list(['ASPERGES']),\n",
       "       list(['ASPERGES']), list(['ASPERGES']), list(['ASPERGES']),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['LIPTON ICE TEA 50CL']), list([]), list([]), list([]),\n",
       "       list([]), list(['ASPERGES']), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['LIPTON ICE TEA 50CL']), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list(['LIPTON ICE TEA 50CL']), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['LIPTON ICE TEA 50CL']), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['AB GPNOX LESCOT 10 L']), list([]), list([]),\n",
       "       list(['1,5L SUMOL ORAN']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list(['ABONNEMENT  13KG']), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list(['ASPIRATEUR LAVAGE COMPLET']),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list(['ASPERGES']),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list(['ASPERGES']),\n",
       "       list(['ASPERGES']), list(['ASPERGES']), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]),\n",
       "       list(['ASPERGES']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]),\n",
       "       list(['1,5L SUMOL ORAN']), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]),\n",
       "       list(['AB GPNOX LESCOT 10 L']), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "       list([]), list([]), list([])], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[\"itemsets\"].apply(lambda x : get_articles_descriptions(x,items,encoder)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform([3408]) in items.ARTICLE_ID.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
